count(word, sort = TRUE) %>%
mutate(freq = n/n()) %>%
filter(word %in% zomby_frequency$word)
zomby_timeline %>%
group_by(hour) %>%
count(word, sort = TRUE) %>%
mutate(freq = n/n()) %>%
filter(word %in% zomby_frequency$word) %>%
# group_by(hour, word) %>%
# summarise(count=n()) %>%
# ungroup() %>%
# complete(word, hour, fill=list(count=0)) %>%
ggplot(aes(x=hour, y=reorder(word, n), group=word, height=freq)) +
geom_ridgeline()
zomby_timeline %>%
group_by(hour) %>%
count(word, sort = TRUE) %>%
mutate(freq = n/n()) %>%
filter(word %in% zomby_frequency$word) %>%
# group_by(hour, word) %>%
# summarise(count=n()) %>%
# ungroup() %>%
# complete(word, hour, fill=list(count=0)) %>%
ggplot(aes(x=hour, y=word, group=word, height=freq)) +
geom_ridgeline()
zomby_timeline %>%
group_by(hour) %>%
count(word, sort = TRUE) %>%
mutate(freq = n/n()) %>%
filter(word %in% zomby_frequency$word)
zomby_timeline %>%
group_by(hour) %>%
count(word, sort = TRUE) %>%
mutate(freq = n/n()*100) %>%
filter(word %in% zomby_frequency$word) %>%
# group_by(hour, word) %>%
# summarise(count=n()) %>%
# ungroup() %>%
# complete(word, hour, fill=list(count=0)) %>%
ggplot(aes(x=hour, y=word, group=word, height=freq)) +
geom_ridgeline()
library(rtweet)
library(tidyverse)
library(lubridate)
library(stringr)
require(tidytext)
require(ggjoy)
## name assigned to created app
appname <- "ewenme"
## api key (example below is not a real key)
key <- "GVy1RpmvRz6zOiKd2agq2X7Rm"
## api secret (example below is not a real key)
secret <- "AhpzLhStSuNZvZjA4wBBUlgwUslkDhGMp6bbZRg73OgPlCWVce"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
zomby_timeline <- get_timeline(user = "ZombyMusic",
n = 10000)
# clean RTs
zomby_timeline <- zomby_timeline %>%
filter(is_retweet == FALSE & is_quote_status == FALSE) %>%
mutate(hour=hour(created_at))
# regex for tweets to keep hashtags etc
replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
# filtering stop words etc. and unnesting each word
zomby_timeline <- zomby_timeline %>%
filter(!str_detect(text, "^RT")) %>%
mutate(text = str_replace_all(text, replace_reg, "")) %>%
unnest_tokens(word, text, token = "regex", pattern = unnest_reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
# counting words, keeping top 20 frequent
zomby_frequency <- zomby_timeline %>%
count(word, sort = TRUE) %>%
mutate(freq = n/length(zomby_timeline)) %>%
top_n(20)
config <- spark_config()
library(tidyverse)
library(sparklyr)
config <- spark_config()
config[["sparklyr.shell.driver-memory"]] <- "2G"
spark_disconnect()
spark_disconnect(sc)
sc <- spark_connect(master = "local", config = config)
spark_read_csv(sc, name = "fl_stops",
path = "~/Documents/R bobs/Stanford Open Policing/FL-clean.csv")
library(tigris)
fl_shp <- counties(state = "FL")
library(sf)
fl_shp <- st_as_sf(fl_shp)
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# read shapefile
fl_shp <- counties(state = "FL")
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# read shapefile
fl_shp <- counties(state = "FL")
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# read shapefile
fl_shp <- counties(state = "FL")
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# # read shapefile
# fl_shp <- counties(state = "FL")
# fl_shp <- st_as_sf(fl_shp)
#
# # install spark
# spark_install()
#
# # creat config to tune memory
# config <- spark_config()
# config[["sparklyr.shell.driver-memory"]] <- "2G"
#
# # connect spark
# sc <- spark_connect(master = "local", config = config)
#
# # read data into spark
# spark_read_csv(sc, name = "fl_stops",
#                path = "~/Documents/R bobs/Stanford Open Policing/FL-clean.csv")
county_counts <- tbl(sc, "fl_stops") %>%
group_by(county_name, county_fips) %>%
summarise(stop_count=n()) %>%
collect()
fl_shp <- st_as_sf(fl_shp)
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# read shapefile
fl_shp <- counties(state = "FL")
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# # read shapefile
# fl_shp <- counties(state = "FL")
# fl_shp <- st_as_sf(fl_shp)
#
# # install spark
# spark_install()
#
# # creat config to tune memory
# config <- spark_config()
# config[["sparklyr.shell.driver-memory"]] <- "2G"
#
# # connect spark
# sc <- spark_connect(master = "local", config = config)
#
# # read data into spark
# spark_read_csv(sc, name = "fl_stops",
#                path = "~/Documents/R bobs/Stanford Open Policing/FL-clean.csv")
unique(fl_shp$COUNTYFP)
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# # read shapefile
# fl_shp <- counties(state = "FL")
# fl_shp <- st_as_sf(fl_shp)
#
# # install spark
# spark_install()
#
# # creat config to tune memory
# config <- spark_config()
# config[["sparklyr.shell.driver-memory"]] <- "2G"
#
# # connect spark
# sc <- spark_connect(master = "local", config = config)
#
# # read data into spark
# spark_read_csv(sc, name = "fl_stops",
#                path = "~/Documents/R bobs/Stanford Open Policing/FL-clean.csv")
unique(fl_shp$COUNTYFP)
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# # read shapefile
# fl_shp <- counties(state = "FL")
# fl_shp <- st_as_sf(fl_shp)
#
# # install spark
# spark_install()
#
# # creat config to tune memory
# config <- spark_config()
# config[["sparklyr.shell.driver-memory"]] <- "2G"
#
# # connect spark
# sc <- spark_connect(master = "local", config = config)
#
# # read data into spark
# spark_read_csv(sc, name = "fl_stops",
#                path = "~/Documents/R bobs/Stanford Open Policing/FL-clean.csv")
fl_shp <- fl_shp %>%
mutate(COUNTYFP=paste0("12", COUNTYFP))
fl_shp <- st_as_sf(fl_shp)
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# # read shapefile
# fl_shp <- counties(state = "FL")
fl_shp <- st_as_sf(fl_shp)
#
# # install spark
# spark_install()
#
# # creat config to tune memory
# config <- spark_config()
# config[["sparklyr.shell.driver-memory"]] <- "2G"
#
# # connect spark
# sc <- spark_connect(master = "local", config = config)
#
# # read data into spark
# spark_read_csv(sc, name = "fl_stops",
#                path = "~/Documents/R bobs/Stanford Open Policing/FL-clean.csv")
fl_shp <- fl_shp %>%
mutate(COUNTYFP=paste0("12", COUNTYFP))
fl_shp <- counties(state = "FL")
fl_shp <- st_as_sf(fl_shp)
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# # read shapefile
fl_shp <- counties(state = "FL")
fl_shp <- st_as_sf(fl_shp)
#
# # install spark
# spark_install()
#
# # creat config to tune memory
# config <- spark_config()
# config[["sparklyr.shell.driver-memory"]] <- "2G"
#
# # connect spark
# sc <- spark_connect(master = "local", config = config)
#
# # read data into spark
# spark_read_csv(sc, name = "fl_stops",
#                path = "~/Documents/R bobs/Stanford Open Policing/FL-clean.csv")
fl_shp <- fl_shp %>%
mutate(COUNTYFP=as.numeric(paste0("12", COUNTYFP))) %>%
left_join(fl_shp, county_counts, by=c("COUNTYFP"="county_fips"))
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180, echo = FALSE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
# # read shapefile
fl_shp <- counties(state = "FL")
class(county_counts)
fl_shp <- fl_shp %>%
mutate(COUNTYFP=as.numeric(paste0("12", COUNTYFP))) %>%
left_join.sf(fl_shp, county_counts, by=c("COUNTYFP"="county_fips"))
county_counts <- as.data.frame(county_counts)
fl_shp <- fl_shp %>%
mutate(COUNTYFP=as.numeric(paste0("12", COUNTYFP))) %>%
left_join.sf(fl_shp, county_counts, by=c("COUNTYFP"="county_fips"))
class(county_counts)
fl_shp <- fl_shp %>%
mutate(COUNTYFP=as.numeric(paste0("12", COUNTYFP))) %>%
left_join.sf(county_counts, by=c("COUNTYFP"="county_fips"))
fl_shp <- fl_shp %>%
mutate(COUNTYFP=as.numeric(paste0("12", COUNTYFP))) %>%
left_join(county_counts, by=c("COUNTYFP"="county_fips"))
ggplot(fl_shp) +
geom_sf(aes(fill = stop_count)) +
scale_fill_viridis("No. of Stops") +
ggtitle("Motorist stops by Police in counties in Florida") +
theme_bw()
devtools::install_github("tidyverse/ggplot2")
library(tidyverse)
library(ggplot2)
unloadNamespace("tidyverse")
detach("tidyverse")
library(ggplot2)
# install latest ggplot2 version
devtools::install_github("tidyverse/ggplot2", force=TRUE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
library(hrbrthemes)
library(scales)
# install spark
spark_install()
# creat config to tune memory
config <- spark_config()
config[["sparklyr.shell.driver-memory"]] <- "2G"
# connect spark
sc <- spark_connect(master = "local", config = config)
# LOAD ----------------------------------------------
# read shapefile
il_shp <- counties(state = "IL")
il_shp <- st_as_sf(fl_shp)
# read data into spark
spark_read_csv(sc, name = "il_stops",
path = "~/Documents/R bobs/Stanford Open Policing/IL-clean.csv")
il_shp <- st_as_sf(il_shp)
county_counts <- tbl(sc, "il_stops") %>%
group_by(county_name, county_fips) %>%
summarise(count=n()) %>%
collect()
View(county_counts)
View(county_counts)
setwd("~/Documents/Github/ewenme")
blogdown::serve_site()
blogdown::serve_site()
# SETUP ---------------------------------------------
# install latest ggplot2 version
# devtools::install_github("tidyverse/ggplot2", force=TRUE)
# load pkgs
library(tidyverse)
library(sparklyr)
library(tigris)
library(sf)
library(hrbrthemes)
library(scales)
library(lubridate)
library(tidycensus)
library(ggalt)
library(gridExtra)
library(ggrepel)
library(ggjoy)
# install spark
spark_install()
# creat config to tune memory
config <- spark_config()
config[["sparklyr.shell.driver-memory"]] <- "2G"
# connect spark
sc <- spark_connect(master = "local", config = config)
# LOAD ----------------------------------------------
# read shapefile
wa_shp <- counties(state = "WA")
wa_shp <- st_as_sf(wa_shp)
# get census data
census_api_key("52dcc3442f47b4e138450637ce5dadb9f444f50c")
wa_pop <- get_acs(geography = "county", state = "WA",
variables = c("B02001_002E",
"B02001_003E", "B03001_003E")) %>%
mutate(race=factor(variable, levels=c("B02001_002", "B02001_003",
"B03001_003"),
labels=c("White", "Black", "Hispanic"))) %>%
group_by(race) %>%
summarise(population=sum(estimate, na.rm=TRUE))
# read data into spark
spark_read_csv(sc, name = "wa_stops",
path = "~/Documents/R bobs/Stanford Open Policing/WA-clean.csv")
# TRANSFORM ----------------------------------------
# get count of officers per month
active_per_month <- tbl(sc, "wa_stops") %>%
mutate(month=month(stop_date), year=year(stop_date)) %>%
group_by(year, month, officer_id) %>%
summarise(stops=n()) %>%
collect()
# local functions
active_per_month %>%
group_by(year, month) %>%
summarise(unique_officers=n_distinct(officer_id)) %>%
ungroup() %>%
mutate(date=as.yearmon(paste(year, month), "%Y %m")) %>%
ggplot(aes(x=date, y=unique_officers)) +
geom_line(alpha=0.5) +
geom_point(alpha=0.5) +
geom_smooth(se=FALSE) +
scale_x_yearmon() +
scale_y_continuous(limits = c(700, 900)) +
theme_ipsum() +
labs(title="Police Officers on Patrol in Washington County",
subtitle="No. of unique officers making stops per month",
x=NULL, y="No. of officers")
stops_per_day <- tbl(sc, "wa_stops") %>%
group_by(stop_date, officer_id) %>%
summarise(stops=n()) %>%
ungroup() %>%
mutate(month=month(stop_date), year=year(stop_date)) %>%
group_by(year, month) %>%
summarise(stops_per_day = mean(stops)) %>%
collect()
# local functions
stops_per_day %>%
ungroup() %>%
mutate(date=as.yearmon(paste(year, month), "%Y %m")) %>%
ggplot(aes(x=date, y=stops_per_day)) +
geom_line(alpha=0.5) +
geom_point(alpha=0.5) +
geom_smooth(se=FALSE) +
scale_x_yearmon() +
theme_ipsum()
# get stop count by county
county_counts <- tbl(sc, "wa_stops") %>%
filter(year(stop_date) >= 2011,
year(stop_date) <= 2015) %>%
group_by(county_name, county_fips) %>%
summarise(count=n()) %>%
ungroup() %>%
mutate(stop_quantile=ntile(count, 5)) %>%
collect()
# summary stats by race
race_stats <- tbl(sc, "wa_stops") %>%
filter(year(stop_date) >= 2011,
year(stop_date) <= 2015,
driver_race %in% c('White', 'Black', 'Hispanic')) %>%
mutate(search=if_else(search_conducted == "TRUE", 1, 0),
hits=if_else(contraband_found == "TRUE", 1, 0)) %>%
group_by(driver_race) %>%
summarise(n_stops=n(),
n_searches=sum(search),
n_hits=sum(hits)) %>%
ungroup() %>%
mutate(search_rate=n_searches / n_stops,
hit_rate=n_hits / n_searches) %>%
collect()
race_stats <- left_join(race_stats, wa_pop, by=c("driver_race"="race"))
# summary stats by gender
gender_stats <- tbl(sc, "wa_stops") %>%
filter(year(stop_date) >= 2011,
year(stop_date) <= 2015) %>%
mutate(search=if_else(search_conducted == "TRUE", 1, 0),
hits=if_else(contraband_found == "TRUE", 1, 0)) %>%
group_by(driver_gender) %>%
summarise(n_stops=n(),
n_searches=sum(search),
n_hits=sum(hits)) %>%
ungroup() %>%
mutate(search_rate=n_searches / n_stops,
hit_rate=n_hits / n_searches) %>%
collect()
# summary stats by demogs
demog_stats <- tbl(sc, "wa_stops") %>%
filter(year(stop_date) >= 2011,
year(stop_date) <= 2015) %>%
mutate(search=if_else(search_conducted == "TRUE", 1, 0),
hits=if_else(contraband_found == "TRUE", 1, 0),
month=month(stop_date)) %>%
group_by(driver_race, driver_gender, driver_age, county_name,
month) %>%
summarise(n_stops=n(),
n_searches=sum(search),
n_hits=sum(hits)) %>%
ungroup() %>%
collect()
# summary stats by times
time_stats <- tbl(sc, "wa_stops") %>%
filter(year(stop_date) >= 2011) %>%
mutate(search=if_else(search_conducted == "TRUE", 1, 0),
hits=if_else(contraband_found == "TRUE", 1, 0)) %>%
group_by(stop_date, stop_time) %>%
summarise(n_stops=n(), n_searches=sum(search), n_hits=sum(hits)) %>%
ungroup() %>%
collect()
# functions in-house
time_stats <- time_stats %>%
mutate(stop_date=ymd(stop_date),
stop_time=hm(stop_time),
stop_time=hour(stop_time))
rmarkdown::render("~/Documents/Github/ewenme/content/blog/2017-07-27-Open_policing_sparklyr_shiny.Rmd")
library(zoo)
rmarkdown::render("~/Documents/Github/ewenme/content/blog/2017-07-27-Open_policing_sparklyr_shiny.Rmd")
race_county <- demog_stats %>%
filter(driver_race %in% c('White', 'Black', 'Hispanic')) %>%
group_by(driver_race, county_name) %>%
summarise(stops=sum(n_stops), searches=sum(n_searches), hits=sum(n_hits)) %>%
ungroup() %>%
mutate(search_rate=searches/stops, hit_rate=hits/searches) %>%
filter(stops >= 150 & county_name != "" & !is.na(county_name))
rmarkdown::render("~/Documents/Github/ewenme/content/blog/2017-07-27-Open_policing_sparklyr_shiny.Rmd")
rmarkdown::render("~/Documents/Github/ewenme/content/blog/2017-07-27-Open_policing_sparklyr_shiny.Rmd")
blogdown::serve_site()
rmarkdown::render("~/Documents/Github/ewenme/content/blog/2017-07-27-Open_policing_sparklyr_shiny.Rmd")
blogdown::serve_site()
rmarkdown::render("~/Documents/Github/ewenme/content/blog/2017-07-27-Open_policing_sparklyr_shiny.Rmd")
blogdown::serve_site()
spark_disconnect_all()
