---
title: "Advocating for Open Policing with sparklyr and Shiny"
author: "ewen"
date: "2017-07-26"
type: "post"
categories: ["R", "Spark", "Shiny"]
tags: ["blogdown", "R", "spark", "shiny"]
excerpt: "Toying around with R's cluster-computing and web development interface household names, to see how they can help us monitor police behaviour with data science."
---

```{r setup, include=FALSE}

# markdown setup
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180)

# install latest ggplot2 version
# devtools::install_github("tidyverse/ggplot2", force=TRUE)

# load pkgs for session
library(tidyverse)
library(hrbrthemes)
library(scales)
library(lubridate)
library(ggalt)
library(gridExtra)
library(ggrepel)
library(sparklyr)

```

I don't have much reason to dip in to big data processing tools in my current working capacity. Luckily for me, I have a (self-indulgent) blog now - perfect place to scratch these sorts of itches, and then share the resultant mishaps with the whole world.

I'd recently been working on a [Shiny](https://shiny.rstudio.com/) app for increasing visibility and engagement with police violence data. It just so happened that I stumbled across [Stanford's Open Policing Project](https://openpolicing.stanford.edu/), with some hefty open data ripe for exploring with big data tools, right after the app's first build was finished. `r tufte::margin_note("I hope to find time to incorporate Stanford's police stops data into my app. At the time of writing, there's still a lot of states missing from their data.")` I figured introducing these things together should make for a smooth read.

<br>

## Stanford's Open Policing Project

Stanford is, slowly but surely, compiling and standardizing data on vehicle and pedestrian stops from law enforcement departments across the U.S. 

<br>

<iframe width="560" height="315" src="https://www.youtube.com/embed/iwOWcuFjNfw" frameborder="0" allowfullscreen></iframe>

<br>

This information is being made freely available -`r tufte::margin_note("The data files for d/l have been heavily cleaned (sounds like some clean-up job, too). The strong-stomached can contact the project team for access to the raw stuff.")` not just the data, but everything needed to reproduce the analysis ([repo in here](https://github.com/5harad/openpolicing)).

All the ingredients for a juicy pet project are in place, then, until I reach this sentence in the project overview:

> *"We’ve already gathered 130 million records from 31 state police agencies..."*

For those of us rocking modest MacBooks est. 2k13, introducing our machines to this magnitude of data may trigger a ripple in the space-time. Without any further ado, I'll move onto my next trick.

<br>

## Introducing spark(lyr)

[sparklyr](http://spark.rstudio.com/) is R Studio's interface to [Apache Spark](https://spark.apache.org/), a pre-eminent open-source cluster-computing framework. The coup de grâce for sparklyr is it's complete [dplyr](https://github.com/hadley/dplyr) back-end, meaning there isn't too much unfamiliar code to see here.

> The what-does-Spark-do TL;DR:
*Spark makes programs run faster by utilising a distributed computing engine for expressive data processing. A useful analogy - now, you're sending one person into a house to find something, whereas a distributed system sends someone into each room of the house and they communicate progress to each other.*

What happens now? First off, the usual CRAN install/load one-two:

```{r sparklyr install, eval=FALSE}
install.packages("sparklyr")
library(sparklyr)
```

Spark also needs to be installed locally. This is as simple as:`r tufte::margin_note("Deploying Spark locally will be as far as this post goes. When you need it, you can find out more about cluster deployment [here](https://spark.apache.org/docs/2.1.1/).")`

```{r spark install, eval=FALSE}
spark_install()
```

Now, there are all kinds of Spark configuration options at hand to change the behaviour of sparklyr and the cluster itself. I won't go into extensive detail on this (Spark do, [here](https://spark.apache.org/docs/latest/spark-standalone.html)) - for now, I'll just initialize a `spark_config` object with just the amount of memory used for the driver process set.

```{r spark config, eval=FALSE}
config <- spark_config()
config[["sparklyr.shell.driver-memory"]] <- "2G"
```

The final step of the sparklyr setup is to  establish a connection to the Spark instance:

```{r spark connect, eval=FALSE}
sc <- spark_connect(master = "local", config = config)
```

`sc` is now acting as a remote dplyr data source to the Spark cluster. In the latest R Studio IDE, you can check things are looking normal, sparklyr-wise:

```{r sparklyr screenshot, echo = FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("https://github.com/rbind/ewenme/blob/master/public/img/2017-07-23-Open_policing_sparklyr_shiny/sparklyr%20screenshot.png")
```

Nice, but, there's not much going on over there at the moment. Enter the data.

<br>

### Back to the Data

Stanford have made the police stops data available in a manageable way, as a series of [data files](https://openpolicing.stanford.edu/data/) (one for each state). I decided to take a look at Washington, which had all common fields available and relatively few data quality issues.`r tufte::margin_note("On the d/l page, a table explains which fields are present in each data file. The GitHub repo's [readme](https://github.com/5harad/openpolicing/blob/master/README.md) has detailed data quality information about each state, which you should review before getting stuck in.")` 

There are two main methods for getting data up into a Spark instance.

- Read data into R as normal, and then use sparklyr's `copy_to` function to copy the data over
- Read data directly into Spark Dataframes using `spark_read_csv` (or another of sparklyr's 'read' functions)

I will be adopting the latter approach. The dataset in question is a not-to-be-sniffed-at 8+ million rows, so I don't want to be doing much with it in-memory in R. So, here we go...

```{r spark read data, eval=FALSE}
spark_read_csv(sc, name = "wa_stops",
               path = "../data/WA-clean.csv")
```

...and, ~five minutes later, it's showtime. The data has been copied into the Spark cluster, and I promise there's no more session prep work.`r tufte::margin_note("Due to the sheer size of the data, I haven't actually pushed it to GitHub. However, you can get it [here](https://openpolicing.stanford.edu/data/)(it's the one called Washington).")`

<br>

### *"Hello dplyr, my old friend"*

We can use all of your favourite dplyr moves to manipulate the data,`r tufte::margin_note("I found that the extended tidyverse family of packages could be used with mixed results. For example, lubridate's *year* function worked OK, but not *wday*. Therefore, some transformations may still need to be done in R. Keep up with updates over at the sparklyr [GitHub repo](https://github.com/rstudio/sparklyr).")` and these computations will take place over in the cluster.

Here's an example of a dplyr transformation that returns a summary of the data grouped on several demographic fields. 

```{r spark dplyr eg, eval=FALSE}

# initiate spark data source
demog_stats <- tbl(sc, "wa_stops") %>%
  # filter 2011-2015 date range
  filter(year(stop_date) >= 2011,
         year(stop_date) <= 2015) %>%
  # make search/hits boolean fields and a month field
  mutate(search=if_else(search_conducted == "TRUE", 1, 0),
         hits=if_else(contraband_found == "TRUE", 1, 0),
         month=month(stop_date)) %>%
  # group data by desired fields
  group_by(driver_race, driver_gender, driver_age, county_name,
           month) %>%
  # summary stats for stops, searches and hits
  summarise(n_stops=n(),
            n_searches=sum(search),
            n_hits=sum(hits)) %>%
  # remove grouping
  ungroup() %>%
  # drag data from spark to R
  collect()

```

Most of this will be familiar to the dplyr-literate. The function you might not be familiar with is `collect`, which is the function that gets sparklyr to drop the query result into our R environment.`r tufte::margin_note("It's generally advisable not to do too much piping (%>%) at once, so you can debug your code adequately. You can use *compute* to store query results in Spark within a temporary table, and take into a subsequent query.")`

```{r spark dplyr cmds, eval=FALSE, include=FALSE}

# summary stats by time fields
time_stats <- tbl(sc, "wa_stops") %>%
  filter(year(stop_date) >= 2011) %>%
  mutate(search=if_else(search_conducted == "TRUE", 1, 0),
         hits=if_else(contraband_found == "TRUE", 1, 0)) %>%
  group_by(stop_date, stop_time) %>%
  summarise(n_stops=n(), n_searches=sum(search), n_hits=sum(hits)) %>%
  ungroup() %>%
  collect()
# functions in-house
time_stats <- time_stats %>%
  mutate(stop_date=ymd(stop_date),
         stop_time=hm(stop_time),
         stop_time=hour(stop_time))

# get count of officers per month
active_per_month <- tbl(sc, "wa_stops") %>%
  mutate(month=month(stop_date), year=year(stop_date)) %>%
  group_by(year, month, officer_id) %>%
  summarise(stops=n()) %>%
  collect()

# summary stats by race
race_stats <- tbl(sc, "wa_stops") %>%
  filter(year(stop_date) >= 2011,
         year(stop_date) <= 2015,
         driver_race %in% c('White', 'Black', 'Hispanic')) %>%
  mutate(search=if_else(search_conducted == "TRUE", 1, 0),
         hits=if_else(contraband_found == "TRUE", 1, 0)) %>%
  group_by(driver_race) %>%
  summarise(n_stops=n(),
            n_searches=sum(search),
            n_hits=sum(hits)) %>%
  ungroup() %>%
  mutate(search_rate=n_searches / n_stops,
         hit_rate=n_hits / n_searches) %>%
  collect()

# scatter of search / hit rates with no. stops as size
race_county <- demog_stats %>%
  filter(driver_race %in% c('White', 'Black', 'Hispanic')) %>%
  group_by(driver_race, county_name) %>%
  summarise(stops=sum(n_stops), searches=sum(n_searches), hits=sum(n_hits)) %>%
  ungroup() %>%
  mutate(search_rate=searches/stops, hit_rate=hits/searches) %>%
  filter(stops >= 150 & county_name != "" & !is.na(county_name))

```

Once necessary data transformations have been done in Spark and collected in R's environment, you can close the connection down, and it's back to R for the analysis work.

```{r spark disconnect, eval=FALSE}
spark_disconnect(sc)
```

<br>

## Monitoring Washington's Police Stops

First, a look at the trends of police stops, searches and 'hits' (searches finding contraband) over time. For comparative purposes, I've used January 2011 as a baseline figure and measured percentage change from this baseline for each of these metrics.

```{r stop trend plot, echo=FALSE}

# stops over time
time_stats %>%
  mutate(ym=floor_date(stop_date, "month")) %>%
  gather(key=type, value=count, n_stops:n_hits) %>%
  group_by(ym, type) %>%
  summarise(count=sum(count)) %>%
  group_by(type) %>%
  mutate(index=count/first(count)) %>%
  ungroup() %>%
  mutate(type=factor(type, levels=c('n_stops', 'n_searches', 'n_hits'),
            labels=c('Stops', 'Searches', 'Hits (contraband found in searches)'))) %>%
  ggplot(aes(x=ym, y=index, colour=type)) +
  geom_line(alpha=0.4) +
  geom_point(alpha=0.4) +
  geom_smooth(se=FALSE) +
  scale_x_date() +
  scale_y_percent(limits = c(0, 2.1)) +
  theme_ipsum() +
  theme(legend.position = "top", legend.title = element_blank()) +
  scale_colour_viridis_d() +
  labs(title="Police stop trends in Washington, 2011-2016",
       caption="data from openpolicing.stanford.edu     @ewen_",
       x=NULL, y="% change from baseline") +
  geom_vline(xintercept = as.Date.character("2015-02-26"), linetype="dashed", alpha=0.8) +
  annotate("text", label = "Marijuana legalisation\n(Washington D.C.)", 
           size = 2.8, x = as.Date.character("2014-12-01"), y = 1.6, angle = 90) +
  geom_vline(xintercept = as.Date.character("2012-12-06"), linetype="dashed", alpha=0.8) +
  annotate("text", label = "Marijuana legalisation\n(Washington State)", 
           size = 2.8, x = as.Date.character("2012-09-06"), y = 1.6, angle = 90)

```

While stops have remained pretty stable over time, there's been a drop-off in searches and (an even bigger one) in hits. That is, apart from the immediate period following legalisation of marijuana possession for adults 21 and over at the end of February 2015 in D.C. - I'm hypothesizing that the police were extra vigilant during this period to make sure such a controversial law change was being followed to the letter.

How has the police force itself fared over time? With the officer ID field, a proxy of the number of active officers on patrol can be established - I've gone with the number of unique officer IDs making at least one stop in a month.

```{r active officers plot, echo=FALSE}

active_per_month %>%
  group_by(year, month) %>%
  summarise(unique_officers=n_distinct(officer_id)) %>%
  ungroup() %>%
  mutate(date=as.yearmon(paste(year, month), "%Y %m")) %>%
  ggplot(aes(x=date, y=unique_officers)) +
  geom_line(alpha=0.5) +
  geom_point(alpha=0.5) +
  geom_smooth(se=FALSE) +
  scale_x_yearmon() +
  scale_y_continuous(limits = c(700, 900)) +
  theme_ipsum() +
  labs(title="Police Officers on Patrol in Washington",
       subtitle="No. of unique officers making stops each month",
       x=NULL, y="No. of officers",
       caption="data from openpolicing.stanford.edu     @ewen_")

```

The force seems to be getting smaller. Notice that there's also a clear seasonal component to the number of officers on patrol - maybe officers don't really want to be out for too long in those winter months.

There are fields in the data that can be used to get at racial disparities and possible bias in police behaviour. For example, the search/hits metrics from earlier can be taken a bit further and considered as rates based on stops and searches respectively. This study of outcomes may indicate discrimination.

```{r search/hit rates, echo=FALSE}
race_stats %>%
  mutate(stop_rate=n_stops/population*10000) %>%
  gather(key=type, value=count, n_stops:stop_rate) %>%
  filter(type %in% c('stop_rate', 'search_rate', 'hit_rate')) %>%
  mutate(type=factor(type, levels=c('stop_rate', 'search_rate', 'hit_rate'),
                     labels=c('Stop rate (per 10,000 population)', 
                              'Search rate (% of stops resulting in a search)', 
                              'Hit rate (% of searches with contraband found)'))) %>% 
  filter(type != "Stop rate (per 10,000 population)") %>%
  ggplot(aes(x=driver_race, y=count, fill=type)) +
  geom_col(position = "dodge") +
  theme_ipsum() +
  theme(legend.position = "top", legend.title = element_blank()) +
  scale_fill_viridis_d() +
  scale_y_percent() +
  coord_flip() +
  labs(y=NULL, x=NULL, title="Police search/contraband seizure rates by race", 
       subtitle="Washington, 2011-2015",
       caption="data from openpolicing.stanford.edu     @ewen_")
```

While Black/Hispanic people are searched more often than Whites when searched, the hit rate (% of searches with contraband found) is lower for these minority groups, demonstrating the discrimination they face in this area.

Is this phenomenon seen across country forces? Using the geographic data fields, it's possible to identify regional trends. 

```{r search/hit scatter, echo=FALSE}

ggplot(race_county) +
  geom_point(aes(x=search_rate, y=hit_rate, size=stops, colour=driver_race), alpha=0.7) +
  theme_ipsum() +
  scale_colour_viridis_d() +
  scale_size_continuous(labels=comma) +
  geom_hline(yintercept = median(race_county$hit_rate), linetype="dotted") +
  geom_vline(xintercept = median(race_county$search_rate), linetype="dotted") +
  scale_y_percent(limits = c(-0.01, 0.8)) +
  scale_x_percent(limits = c(-0.001, 0.1)) +
  labs(title="Police search/contraband seizure rates by race",
       subtitle="Washington counties, 2011-2015",
       x= "% of stops resulting in a search", y="% of searches finding contraband",
       caption="data from openpolicing.stanford.edu     @ewen_",
       size="No. of stops", colour="Driver's race") +
  geom_label_repel(data = filter(race_county, 
                           hit_rate > 0.4 | search_rate > 0.08),
            aes(x=search_rate, y=hit_rate, label=county_name),
            segment.colour = "black", segment.alpha = 0.5, nudge_y = 0.1)

```

The rate of searches is consistently higher amongst Black and Hispanic people, compared to Whites.`r tufte::margin_note("Quick word of warning - hit rates can be misleading. While a good indicator of discrimination, it's not quite enough to infer racial bias. For example, suppose there are just two types of white drivers with either a 5% or 75% likelihood of carrying contraband. Suppose there are also just two types of black drivers: some black drivers have a 5% chance of carrying contraband, and the others have a 50% chance of carrying contraband. If a fair police officer only searches drivers with at least a 10% chance of carrying something illegal, the white hit rate would be 75% and the black hit rate would be 50%. The officer used the same standard to search each driver, and so did not discriminate, even though the hit rates differ.")`  The fact that there are a number of instances when the 'hit rate' is close to zero amongst searches of minority groups suggests that the threshold for searches, or standard of evidence needed to initiate a search, is lower than for whites.

This was a taste of how Spark can be utilized to power large-scale analyses of police behaviour and profiling - I hope to revisit the data as more states join in (and perhaps do a comparison with our forces across the pond, someday).

<br>

## Introducing polMonitor (and Mapping Police Violence)

polMonitor is a related pet project that has seen the light of day thanks to a herculean effort by [Mapping Police Violence](https://mappingpoliceviolence.org/) to compile data about police killings from several disparate sources (namely, FatalEncounters.org, the U.S. Police Shootings Database and KilledbyPolice.net).

With all of this work going in to data collection, I was inspired to do whatever I could to make the data accessible and engaged with by more people, and ensure more accountable policing. To that end, I developed a Shiny app (now [hosted over at shinyapps](https://ewenme.shinyapps.io/polMonitor/)), or an interactive space for folks to immerse themselves in this data. Go see for yourself, and check the repo [here](https://github.com/ewenme/Vinylspotting).

<br>

```{r polmonitor anim, echo = FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("https://github.com/ewenme/polMonitor/blob/master/example.gif")
```

<br>

I'm not going to delve into the ins and outs of Shiny development, but I wanted to quickly mention a couple of helping hands I found with this one.

<br>

- Going that extra 10 yards with [shinyjs](http://deanattali.com/shinyjs/): Dean Attali's Shiny package lets dummies like me do neat JavaScript tricks all over the shop. I particularly digged the `hidden` function, which lets you choose bits of the app for the user to hide when not particularly useful.
- The GIS dream-team of [sf](https://github.com/r-spatial/sf) and [tidycensus](https://github.com/walkerke/tidycensus): Boy did this package combo come through for me when it was a choropleth map situation. 

<br>

## Wrap-up

I set out to learn a bit more about Spark and Shiny, and put a spotlight on critical policing issues in the process. The trickle down of new technologies can be slower to reach these kinds of spaces. I think it's our job to ensure that as powerful, but often biased algorithms continue to creep into the targeting of disenfranchised groups, there is a pragmatic effort to expose injustice in these systems and encourage open, accountable policing through good data science. I hope writing this helps remind me of this ongoing responsibility to pursuing these causes.