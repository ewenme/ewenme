---
title: "Meet Dumb Hardwax: Markov Chains & Twitter Bots in the Cloud, in R"
date: "2017-09-07"
type: "post"
tags: ["R", "Bots", "Web Scraping", "Azure", "Markov Chains"]
---

```{r setup, include=FALSE}

# markdown setup
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180)

# load pkgs for session
library(tidyverse)
library(rvest)
library(stringr)

```

Twitter bots have gotten a fairly bad rap, recently [(often with good reason)](http://uk.businessinsider.com/twitter-russia-investigation-should-look-at-trump-interaction-with-bots-2017-10?r=US&IR=T). When they're done right, a genuinely quirky robot can cut through a feed full of humans with beautiful tidbits. God bless [@tinycarebot](https://twitter.com/tinycarebot).

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">ðŸ’Ÿ: please dont forget to text back your friends</p>&mdash; here&#39;s your reminder (@tinycarebot) <a href="https://twitter.com/tinycarebot/status/916319972112175105?ref_src=twsrc%5Etfw">October 6, 2017</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<br>

Another thing I like reading is [Hardwax](https://hardwax.com/), a hugely influential Berlin record store, get excited about their new stock. Their snappy, idiosyncratic desciptions of new music is steeped in electronic music folklore. Gems like this are peppered all over the site:

> "Like Dancehall from cyberspace - awesomely fresh & fearless & full of Grime affinities"

I could flick throught these all day, but as a Londoner I'm usually gonna pick up records from the local stores. This means I don't get around to checking these as much as I should, which got me thinking - I wish I could have Hardwax reviews in the Twitter feed. They even fit in to 140 characters most of the time...

I'd also been meaning to have a go at generating pseudo random text with Markov chains, after coming across [Roel's post here](http://rmhogervorst.nl/cleancode/blog/2017/01/21/markov-chain.html). For those that don't know about this type of chain, [this](http://setosa.io/ev/markov-chains/) is a wicked visual intro, but in short they are mathematical systems that describe the probabilities of moving from one "state" (or set of values) to another. Could there be potential to subvert this principle for the purpose of knitting together words to form my own fake Hardwax reviews? 

So, a bot that spits out pseudo-Hardwax reviews, just for my sadistic enjoyment. Let's get it.

<br>

## Gettin' in to a scrape

First up, I went straight to the Hardwax webshop to see if I could get hold of the review/description text accompanying releases on there. This could serve as the corpus of text from which we can base our markov chain review generator on. Here's what the release pages look like, over at Hardwax's site:

```{r site pic, out.width = "100%", fig.align = "center"}
knitr::include_graphics("/img/2017-10-03-Meet_Dumb_Hardwax/hardwax site.png")
```

Those bits circled in red? *Those* are the Hardwax reviews. I was able to put together a fairly simple function (leaning heavily on `rvest`) to scrape them.

```{r hardwax scrape}

# web scraper for hardwax reviews
hardwax_scrape <- function(page, no) {
  
  # construct url
  x <- paste0("https://hardwax.com/", page, "/?page=", no, "&paginate_by=50")
  
  # scrape reviews
  reviews <- x %>%
    read_html() %>%
    html_nodes("p") %>%
    html_text()
  
  return(reviews)
}
```

This URL structure could be applied for each section/genre on the site, like the latest weeks new ish:

```{r scrape eg}

# scrape news
lapply(seq_along(1:2), hardwax_scrape, page="this-week") %>% unlist() %>% head()

```

```{r scrape rest, echo=FALSE}

# scrape last weeks news
last_news <- lapply(seq_along(1:3), hardwax_scrape, page="last-week") %>% unlist()

# scrape back in stock
back_in_stock <- lapply(seq_along(1:15), hardwax_scrape, page="back-in-stock") %>% unlist()

# scrape downloads
downloads <- lapply(seq_along(1:202), hardwax_scrape, page="downloads") %>% unlist()

# get electro
electro <- lapply(seq_along(1:11), hardwax_scrape, page="electro") %>% unlist()

# get grime
grime <- lapply(seq_along(1:22), hardwax_scrape, page="grime") %>% unlist()

# get techno
techno <- lapply(seq_along(1:66), hardwax_scrape, page="techno") %>% unlist()

# get house
house <- lapply(seq_along(1:42), hardwax_scrape, page="house") %>% unlist()

# bind all of these
reviews <- c(news, last_news, back_in_stock, downloads, electro, grime, techno, house)
```

Once the various sections were scraped, some data cleaning procedures (remove releases with no reviews, reviews longer than 140 characters, or duplicate reviews) ensured the reviews were fit for purpose to head on to the next stage.[^fullcode]

```{r clean, echo=FALSE}

# check strings 140 chrs at most
reviews <- reviews[str_length(reviews) <= 140]

# remove empty or NA strings
reviews <- reviews[!is.na(reviews) & reviews != ""]

# remove duplicates
reviews <- reviews[!duplicated(reviews)]

reviews <- as_data_frame(reviews)
```

<br>

## Preppin' the Text  

Now we're entering text mining territory, it's time to call on the might of `tidytext` to bring our body of text into forms suitable for Markov Chain text generation. A few wise steps should see us through.

1. Counts: To aid the probabilistic elements of markov chain text generation, we need an understanding of how many times words appear, in different contexts:

    - No. times words appear in corpus (all webstite review text)
    - No. times words appear at the beginning of a review (herein known as 'openers')
    - No. times words precede commas
    
```{r counts, echo=FALSE}
# get unique words
word_counts <- reviews %>%
  unnest_tokens(word, value, token = "ngrams", to_lower = TRUE, n = 1) %>%
  count(word, sort = TRUE) %>% filter(word != "")

# get sentence openers
opener_counts <- str_extract(reviews$value, '\\w*') %>% 
  str_to_lower() %>% 
  as_data_frame() %>%
  count(value, sort = TRUE) %>% 
  rename(word=value) %>% 
  filter(word != "")

# get words preceding commas
comma_precede <- str_split_fixed(reviews$value, ',', 5)  %>% 
  as_data_frame() %>% 
  filter(`V1` != "")

a <- comma_precede %>% filter(V2 != "") 
b <- a %>% filter(`V3` != "") %>% select(`V2`)
c <- a %>% filter(`V4` != "") %>% select(`V3`)
d <- a %>% filter(`V5` != "") %>% select(`V4`)

comma_precede <- c(a$V1, b$V2, c$V3, d$V4) %>% as_data_frame()

# get last words preceding commas
comma_precede$value <- word(comma_precede$value, -1)

# make lower case
comma_precede$value <- tolower(comma_precede$value)

# get word proceeding comma counts
comma_precede_counts <- count(comma_precede, value, sort = TRUE) %>% rename(comma_n=n)

# join to abs word counts
word_counts <- left_join(word_counts, comma_precede_counts, by=c("word"="value"))
opener_counts <- left_join(opener_counts, comma_precede_counts, by=c("word"="value"))

# probability of comma after word
word_counts$comma_prob <- round((word_counts$comma_n / word_counts$n) * 100)
opener_counts$comma_prob <- round((opener_counts$comma_n / opener_counts$n) * 100)
word_counts$comma_prob[is.na(word_counts$comma_prob)] <- 0
opener_counts$comma_prob[is.na(opener_counts$comma_prob)] <- 0
```

2. Ngrams:

    - Bigrams (pairs of consecutive words)
    - Trigrams (groups of three consecutive words)
    
```{r ngrams, echo=FALSE}
# create bigrams
bigrams <- reviews %>%
  unnest_tokens(bigram, value, token = "ngrams", to_lower = TRUE, n = 2) %>% 
  # separate bigram col
  separate(bigram, c("word1", "word2"), sep = " ")

# new bigram counts:
bigram_counts <- bigrams %>% 
  count(word1, word2, sort = TRUE)

# create bigrams
trigrams <- reviews %>%
  unnest_tokens(trigram, value, token = "ngrams", to_lower = TRUE, n = 3) %>% 
  # separate bigram col
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

# new bigram counts:
trigram_counts <- trigrams %>% 
  count(word1, word2, word3, sort = TRUE)
```

<br>

## 

<br>

[^fullcode]: To keep the post concise I don't show all of the code, especially code that generates figures. But you can find the full code [here](https://github.com/rbind/ewenme/blob/master/content/blog/2017-09-07-Dissecting_European_Football_Transfers.Rmd).