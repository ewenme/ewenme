---
title: "Advocating for Open Policing with sparklyr and Shiny"
author: "ewen"
date: "23/07/2017"
type: "post"
categories: ["R", "Spark", "Shiny"]
tags: ["blogdown", "R", "spark", "shiny"]
excerpt: "Toying around with R's cluster-computing and web development interface household names, to see how they can help us monitor police practices with data science."
---

```{r setup, include=FALSE}

# markdown setup
knitr::opts_chunk$set(cache=TRUE, echo = TRUE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180)

# install latest ggplot2 version
# devtools::install_github("tidyverse/ggplot2", force=TRUE)

# load pkgs for session
library(tidyverse)
library(hrbrthemes)
library(scales)
library(lubridate)
library(ggalt)
library(gridExtra)
library(ggrepel)
library(sparklyr)

```

I don't have much reason to dip in to big data processing tools in my current working capacity. Luckily for me, I have a (self-indulgent) blog now - perfect place to scratch these sorts of itches, and then share the resultant mishaps with the whole world.

I'd recently been working on a [Shiny](https://shiny.rstudio.com/) app for increasing visibility and engagement with police violence data. It just so happened that I stumbled across [Stanford's Open Policing Project](https://openpolicing.stanford.edu/), with some hefty open data ripe for exploring with big data tools, right after the app's first build was finished. `r tufte::margin_note("I hope to find time to incorporate Stanford's police stops data into my app. At the time of writing, there's still a lot of states missing from their data.")` I figured introducing these things together should make for a smooth read.

<br>

## Stanford's Open Policing Project

Stanford is, slowly but surely, compiling and standardising data on vehicle and pedestrian stops from law enforcement departments across the U.S. 

<br>

<iframe width="560" height="315" src="https://www.youtube.com/embed/iwOWcuFjNfw" frameborder="0" allowfullscreen></iframe>

<br>

This information is being made freely available -`r tufte::margin_note("The data files for d/l have been heavily cleaned (sounds like some clean-up job, too). The strong-stomached can contact the project team for access to the raw stuff.")` not just the data, but everything needed to reproduce the analysis ([repo in here](https://github.com/5harad/openpolicing)).

All the ingredients for a juicy pet project are in place, then, until I reach this sentence in the project overview:

> *"We’ve already gathered 130 million records from 31 state police agencies..."*

For those of us rocking modest MacBooks est. 2k13, introducing our machines to this magnitude of data may trigger a ripple in the space-time. Without any further ado, I'll move onto my next trick.

<br>

## Introducing spark(lyr)

[sparklyr](http://spark.rstudio.com/) is R Studio's interface to [Apache Spark](https://spark.apache.org/), a pre-eminent open-source cluster-computing framework. The coup de grâce for sparklyr is it's complete [dplyr](https://github.com/hadley/dplyr) back-end, meaning you won't see too much unfamiliar code here.

> The what-does-Spark-do TL;DR:
*Spark makes programs run faster by utilising a distributed computing engine for expressive data processing. A useful analogy - now, you're sending one person into a house to find something, whereas a distributed system sends someone into each room of the house and they communicate progress to each other.*

What happens now? First off, the usual CRAN install/load one-two:

```{r sparklyr install, eval=FALSE}
install.packages("sparklyr")
library(sparklyr)
```

You'll also need to install Spark locally. This is as simple as:`r tufte::margin_note("Deploying Spark locally will be as far as this post goes. When you need it, you can find out more about cluster deployment [here](https://spark.apache.org/docs/2.1.1/).")`

```{r spark install, eval=FALSE}
spark_install()
```

Now, there are all kinds of Spark configuration options at your disposal to change the behaviour of sparklyr and the cluster itself. I won't go into extensive detail on this (Spark do, [here](https://spark.apache.org/docs/latest/spark-standalone.html)) - for now, I'll just initialize a `spark_config` object with just the amount of memory used for the driver process set.

```{r spark config, eval=FALSE}
config <- spark_config()
config[["sparklyr.shell.driver-memory"]] <- "2G"
```

The final step of the sparklyr setup is to  establish a connection to our Spark instance:

```{r spark connect, eval=FALSE}
sc <- spark_connect(master = "local", config = config)
```

`sc` is now acting as a remote dplyr data source to the Spark cluster. In the latest R Studio IDE, you can check things are looking normal, sparklyr-wise:

```{r sparklyr screenshot, echo = FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("/img/2017-07-23-Open_policing_sparklyr_shiny/sparklyr screenshot.png")
```

Nice, but, there's not much going on over there at the moment...

<br>

### Back to the Data

Stanford have made the police stops data available in a manageable way, as a series of [data files](https://openpolicing.stanford.edu/data/) (one for each state). I decided to take a look at Washington, which had all common fields available and relatively few data quality issues.`r tufte::margin_note("On the d/l page, a table explains which fields are present in each data file. The GitHub repo's [readme](https://github.com/5harad/openpolicing/blob/master/README.md) has detailed data quality information about each state, which you should review before getting stuck in.")` 

There are two main methods for getting data up into a Spark instance.

- Read data into R as normal, and then use sparklyr's `copy_to` function to copy the data over
- Read data directly into Spark Dataframes using `spark_read_csv` (or another of sparklyr's 'read' functions)

I will be adopting the latter approach. The dataset in question is a weighty 8+ million rows, so I don't want to be doing much with it in-memory in R. So, here we go...

```{r spark read data, eval=FALSE}
spark_read_csv(sc, name = "wa_stops",
               path = "../data/WA-clean.csv")
```

...and, ~five minutes later, we're in business. The data has been copied into the Spark cluster, and I promise there's no more prep work.

<br>

### *"Hello dplyr, my old friend"*

We can use all of your favourite dplyr moves to manipulate the data,`r tufte::margin_note("I found that the extended tidyverse family of packages could be used with mixed results. For example, lubridate's `year` function worked OK, but not `wday`. Therefore, some transformations may still need to be done in R. Keep up with updates over at the sparklyr [GitHub repo](https://github.com/rstudio/sparklyr).")` and these computations will take place over in the cluster.

Here's an example of a dplyr transformation that returns a summary of the data grouped on several demographic fields. 

```{r spark dplyr eg, eval=FALSE}

# initiate spark data source
demog_stats <- tbl(sc, "wa_stops") %>%
  # filter 2011-2015 date range
  filter(year(stop_date) >= 2011,
         year(stop_date) <= 2015) %>%
  # make search/hits boolean fields and a month field
  mutate(search=if_else(search_conducted == "TRUE", 1, 0),
         hits=if_else(contraband_found == "TRUE", 1, 0),
         month=month(stop_date)) %>%
  # group data by desired fields
  group_by(driver_race, driver_gender, driver_age, county_name,
           month) %>%
  # summary stats for stops, searches and hits
  summarise(n_stops=n(),
            n_searches=sum(search),
            n_hits=sum(hits)) %>%
  # remove grouping
  ungroup() %>%
  # drag data from spark to R
  collect()

```

Most of this will be familiar to the dplyr-literate. The function you might not be familiar with is `collect`, which is the function that gets sparklyr to drop the query result into our R environment.`r tufte::margin_note("It's generally advisable not to do too much piping (%>%) at once, so you can debug your code adequately. You can use `compute` to store query results in Spark within a temporary table, and take into a subsequent query.")`

```{r spark dplyr cmds, eval=FALSE, include=FALSE}

# summary stats by time fields
time_stats <- tbl(sc, "wa_stops") %>%
  filter(year(stop_date) >= 2011) %>%
  mutate(search=if_else(search_conducted == "TRUE", 1, 0),
         hits=if_else(contraband_found == "TRUE", 1, 0)) %>%
  group_by(stop_date, stop_time) %>%
  summarise(n_stops=n(), n_searches=sum(search), n_hits=sum(hits)) %>%
  ungroup() %>%
  collect()
# lubridate in-house
time_stats <- time_stats %>%
  mutate(stop_date=ymd(stop_date),
         stop_time=hm(stop_time),
         stop_time=hour(stop_time))

```

Once necessary data transformations have been done in Spark and collected in R's environment, you can close the connection down, and it's back to R for the analysis work.

```{r spark disconnect, eval=FALSE}
spark_disconnect(sc)
```

<br>

## Monitoring Washington's Police Stops

First, let's look at the trends of police stops, searches and 'hits' (searches finding contraband) over time. For comparative purposes, I've used January 2011 as a baseline figure and measured percentage change from this baseline for each of these metrics.

```{r stop trend plot, echo=FALSE}

# stops over time
time_stats %>%
  mutate(ym=floor_date(stop_date, "month")) %>%
  gather(key=type, value=count, n_stops:n_hits) %>%
  group_by(ym, type) %>%
  summarise(count=sum(count)) %>%
  group_by(type) %>%
  mutate(index=count/first(count)) %>%
  ungroup() %>%
  mutate(type=factor(type, levels=c('n_stops', 'n_searches', 'n_hits'),
            labels=c('Stops', 'Searches', 'Hits (contraband found in searches)'))) %>%
  ggplot(aes(x=ym, y=index, colour=type)) +
  geom_line(alpha=0.4) +
  geom_point(alpha=0.4) +
  geom_smooth(se=FALSE) +
  scale_x_date() +
  scale_y_percent(limits = c(0, 2.1)) +
  theme_ipsum() +
  theme(legend.position = "top", legend.title = element_blank()) +
  scale_colour_viridis_d() +
  labs(title="Police stop trends in Washington, 2011-2016",
       caption="data from openpolicing.stanford.edu     @ewen_",
       x=NULL, y="% change from baseline") +
  geom_vline(xintercept = as.Date.character("2015-02-26"), linetype="dashed", alpha=0.8) +
  annotate("text", label = "Marijuana legalisation", 
           size = 2.5, x = as.Date.character("2015-01-01"), y = 1.6, angle = 90)

```

While stops have remained pretty stable over time, there's been a drop-off in searches and (an even bigger one) in hits. That is, apart from the immediate period following legalisation of marijuana possession for adults 21 and over at the end of February 2015 - I'm hypothesising that the police were extra vigilant during this period to make sure such a big law change was being followed to the letter.