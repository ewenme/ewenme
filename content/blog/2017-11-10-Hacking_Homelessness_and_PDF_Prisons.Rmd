---
title: "Hacking Homelessness & PDF Prisons with hexmapr & tabulizer"
date: "2017-11-10"
type: "post"
tags: ["R", "Mapping", "PDF", "Homelessness"]
---

```{r setup, include=FALSE}

# markdown setup
knitr::opts_chunk$set(cache=TRUE, echo = FALSE, tidy = TRUE, warning = FALSE, message = FALSE, out.width = '100%', dpi = 180)

# load pkgs
library(plyr)
library(stringr)
library(tidyverse)
library(tabulizer)
library(hexmapr)
library(sf)
library(magick)

```

Shout-out to the [Monday Morning Data Science](http://us16.campaign-archive2.com/home/?u=5ea551600fcdf84334e5aa6b0&id=26c0b7221a) mailout from John Hopkins. Making my way through this week's (#41) edition, I hit upon quite the eulogy concerning a new R package: 

> "The most compelling data visualization libraries Iâ€™ve seen in awhile...you have to see it to believe it"

You had me at compelling (I'm not sure *who* had me exactly, this was the only entry w/o a name attached to it `r emo::ji("thinking")`).

## Hex Democratization 

Enter [`hexmapr`](https://github.com/sassalley/hexmapr) and a solution to the perennial information communication problem of plotting polygons of different sizes, that's equal parts functional and beautiful. In the link above the author, Joseph Bailey, eloquently explains the motivations behind this package for automatic transformations of geospatial polygons into regular and hexagonal grids. In short, a grid system ensures a fairer representation of spatial data (while retaining geography) and an automated implementation means this is suitable to use in less commonly studied places. On both counts, this is a wicked example of building tools that democratize good data science. `r emo::ji("hands")`

Next, I just needed a good reason to wheel it out (aside from sharing with y'all). 

## Full Metal Data

[Combined Homelessness and Information Network (CHAIN)](https://www.mungos.org/work-with-us/chain/) is the foremost project collecting data about homelessness in London. Gaining direct access to the database is strictly limited to those working directly with rough sleepers. However, CHAIN reports are published on the Greater London Authority's (GLA) [London Datastore](https://data.london.gov.uk/dataset/chain-reports). The full annual report does contain data on the spatial distribution of rough sleepers, but it's only available in PDF format. `r emo::ji("skull")`

```{r, out.width = "100%", fig.align = "center", echo=FALSE}

knitr::include_graphics("/img/2017-11-10-Hacking_Homelessness_and_PDF_Prisons/file_extensions.png")
```

This xkcd cartoon is in good humour, but there's a lot of truth in this - PDF's have risen to represent a kind of absolute truth, embodied by their relative impenetrability from outside influence (this includes pesky data scientists). This all-but-closed system becomes a big problem when we want to extract the 'open' data holed up inside. (Enter next R package to save the day, stage left).

## Introducing tabulizer AKA Governments, Lock Up Your PDFs

Actual footage of [`tabulizer](https://github.com/ropensci/tabulizer), an rOpenSci project dreamed up by Thomas Leeper, encountering impervious PDF #478:

```{r, out.width = "100%", fig.align = "center", echo=FALSE}

knitr::include_graphics("/img/2017-11-10-Hacking_Homelessness_and_PDF_Prisons/oreally.jpg")
```

As the [rOpenSci release](https://ropensci.org/blog/2017/04/18/tabulizer/) tells, `tabulizer` gives us R mortals the power of the [tabula-java library](https://github.com/tabulapdf/tabula-java) for extracting tables from PDF files (which, in turn, powers [Tabula](http://tabula.technology/), which you might have come across). You should dig in to that release blog (and associated GitHub repo) for the juicy inner workings, but I'll show it off very shortly.

I should say that I hit some snags trying to get this package fully operational (again, detailed in the package release), owing to the dependency on Java. I will just say that these links were able to get me on my way (as a Mac user) AND it was worth it:

- [Relinking Java & rJava](https://stackoverflow.com/questions/35179151/cannot-load-r-xlsx-package-on-mac-os-10-11/36045290)
- [rJava fix after OSX upgrade](https://stackoverflow.com/questions/30738974/rjava-load-error-in-rstudio-r-after-upgrading-to-osx-yosemite)
- [`tabulizer` issues page](https://github.com/ropensci/tabulizer/issues?utf8=%E2%9C%93&q=is%3Aissue)

## Where the Magic Happens

With new friends in tow, it's about to go down on this PDF. Here's an idea of what we're working with (Eeyore sad at PDFs courtesy of [`magick`](https://cran.r-project.org/web/packages/magick/vignettes/intro.html), if you were wondering):

```{r screenshot}
# read screenshot and eeyore
scrn <- image_read('~/Documents/Github/ewenme/static/img/2017-11-10-Hacking_Homelessness_and_PDF_Prisons/report_scrn.png')
eeyore <- image_read('https://upload.wikimedia.org/wikipedia/en/7/70/Eeyore.gif')

# composing
scrn_eeyore <- image_scale(image_background(eeyore, "none"), "x150")
image_composite(image_scale(scrn, "x400"), scrn_eeyore, offset = "+550+30")
```

To get the data in this table out of it's prison, we use `extract_tables()`:

```{r extract, echo=TRUE}
library(tabulizer)

homeless_spatial_tab <- extract_tables(file = "https://files.datapress.com/london/dataset/chain-reports/2017-06-30T09:03:07.84/Greater%20London%20full%202016-17.pdf",
                                       pages = 19)

homeless_spatial_tab

```

We simply fed this two arguments - the filepath of the report and the page number with our table of interest - and returned a list object of length one, containing a character matrix. The resultant object needs some light wrangling to get it just right:

```{r wrangle, echo=TRUE}

# specify table list item
homeless_spatial_tab <- homeless_spatial_tab[[1]]

# remove unneeded rows and cols
homeless_spatial_tab <- homeless_spatial_tab[c(-2, -15, -37), -6:-7]

# get col names
col_names <- homeless_spatial_tab[1, ]

# create dataframe
homeless_spatial_tab <- data.frame(homeless_spatial_tab[-1, ])

# set colnames
colnames(homeless_spatial_tab) <- col_names

library(tidyverse)

# set col types
homeless_spatial_tab <- mutate_at(homeless_spatial_tab, c("2013/14", "2014/15", "2015/16", "2016/17"), as.character)
homeless_spatial_tab <- mutate_at(homeless_spatial_tab, c("2013/14", "2014/15", "2015/16", "2016/17"), as.numeric)

homeless_spatial_tab
```

Eeyore is joyful at this sight, mark my words. How effortless was that? This data is ripe for mapping. Speaking of which...

## Bring in the Maps

Back to `hexmapr` stuff. For starters, let's load in the shapefiles we need (note: the GitHub repo's example is actually on London, too, so I won't dwell on every similar detail):

```{r hex_load, echo=TRUE}
library(hexmapr)

# get spatial polygons
input_file <- system.file('extdata', 'london_LA.json', package='hexmapr')
original_shapes <- read_polygons(input_file)

# get polygon details
original_details <- get_shape_details(original_shapes)

```

In the case of spatial joins, I've found [`sf`](https://github.com/r-spatial/sf)(simple features) incredibly intuitive and so I'll employ this tactic now to merge the distinct datasets together. I did notice some differences in the names given to certain London Boroughs (mainly the use of '&'/'and') which will need addressing as well.

```{r spatial_join, echo=TRUE}
# rename homeless data boroughs strings not matching hexmapr shp
homeless_spatial_tab$Borough <- str_replace(homeless_spatial_tab$Borough, pattern = "Richmond",
                                            replacement = "Richmond upon Thames")
homeless_spatial_tab$Borough <- str_replace(homeless_spatial_tab$Borough, 
                                            pattern = "&",
                                            replacement = "and")

library(sf)

# turn hexmapr shapefile into sf format
original_shapes_sf <- st_as_sf(original_shapes)

# join homeless data
original_shapes_sf <- left_join(original_shapes_sf, homeless_spatial_tab, by=c("NAME"="Borough"))

```

We're about ready to put plot to paper. First, let's take a look at a traditional, real space assignment of the area.

```{r traditional_map, echo=TRUE}

# get coords
coords <- original_shapes_sf %>%
  # find polygon centroids (sf points object)
  st_centroid %>%
  # extract the coordinates of these points as a matrix
  st_coordinates

# insert centroid long and lat fields as attributes of polygons
original_shapes_sf$long <- coords[,1]
original_shapes_sf$lat <- coords[,2]

# get percentage change from baseline
original_shapes_sf$pct_chg <- (original_shapes_sf$`2016/17` - original_shapes_sf$`2013/14`) / original_shapes_sf$`2013/14` * 100

# traditional map
ggplot(original_shapes_sf) +
  geom_sf(aes(fill = pct_chg)) +
  geom_text(aes(long, lat, label=str_sub(NAME, 1, 4)), alpha = 0.75, size = 2.5, color = 'white') +
  coord_sf() +
  scale_fill_viridis_c() +
  theme_void()

```

The metric used here represents % change in 2016/17 rough sleepers compared to 2013/14. Barking is highest (14 up to 49), but the eye and mind may be more drawn to some of it's bigger neighbours (e.g. Havering). 

Algorithmic tesselation is used to generate possible grid layouts for the data (as explained in the GitHub repo - I'll just be demo-ing the hexes, but the same can be done with regular grids):

```{r seeds}

par(mfrow=c(2,3), mar = c(0,0,2,0))
for (i in 1:6){
new_cells <-  calculate_cell_size(original_shapes, original_details,0.03, 'hexagonal', i)
plot(new_cells[[2]], main = paste("Seed",i, sep=" "))
}

```

I'm quite partial to #3. Manoeuvreing from real space geography to this grid involves an implementation of the [hungarian algorithm](https://github.com/RcppCore/rcpp-gallery/blob/gh-pages/src/2013-09-24-minimal-assignment.cpp). All you have to do is `calculate_cell_size()` with the grid (seed) of choice and `assign_polygons()` to the shapefile containing the original geography. From there, the steps to visualisation are identical to previous ones:

```{r hex_map}

new_cells_hex <-  calculate_cell_size(original_shapes, original_details, 0.03, 'hexagonal', 3)
resulthex <- assign_polygons(original_shapes, new_cells_hex)

# turn hexmapr shapefile into sf format
resulthex_sf <- st_as_sf(resulthex)

# join homeless data
resulthex_sf <- left_join(resulthex_sf, homeless_spatial_tab, by=c("NAME"="Borough"))

# get coords
coords <- resulthex_sf %>%
  # find polygon centroids (sf points object)
  st_centroid %>%
  # extract the coordinates of these points as a matrix
  st_coordinates

# insert centroid long and lat fields as attributes of polygons
resulthex_sf$long <- coords[,1]
resulthex_sf$lat <- coords[,2]

# get percentage change from baseline
resulthex_sf$pct_chg <- (resulthex_sf$`2016/17`- resulthex_sf$`2013/14`) / resulthex_sf$`2013/14` * 100

# hex plot
ggplot(resulthex_sf) +
  geom_sf( aes(fill = pct_chg)) +
  geom_text(aes(long, lat, label=str_sub(NAME, 1, 4)), size = 3, color = 'white') +
  scale_fill_viridis_c() +
  coord_sf() +
  theme_void() +
  theme(
    text = element_text(size = 7),
    plot.title = element_text(size = 11, color = "#1c5074", hjust = 0, vjust = 2, face = "bold"), 
    plot.subtitle = element_text(size = 8, color = "#3474A2", hjust = 0, vjust = 0),
    axis.ticks = element_blank(), 
    legend.direction = "vertical", 
    legend.position = "right",
    plot.margin = margin(1, 1, 1, 1, 'cm'),
    legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm")
  ) 

```

